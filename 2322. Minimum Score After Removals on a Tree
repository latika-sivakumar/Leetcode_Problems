# - The problem is to minimize the difference between the maximum and minimum XOR values
#   when two edges are removed from a tree.
# - A DFS is used to compute:
#     1. XOR of all nodes in each subtree (subtree_xor).
#     2. Descendants of each node (to detect overlapping subtrees).
# - For every pair of nodes (i, j), we consider three cases:
#     - One is a descendant of the other.
#     - They are in completely separate subtrees.
# - Based on the structure, we compute the three resulting XORs after removing two edges.
# - The score is the difference between max and min of the three XORs.
# - The goal is to find the minimum such score.

# Time Complexity: O(n^2), due to checking all node pairs after a single DFS traversal.

# Space Complexity: O(n), for storing the graph, subtree XORs, and descendants.

import collections

class Solution:
    def minimumScore(self, nums: list[int], edges: list[list[int]]) -> int:
        n = len(nums)
        graph = collections.defaultdict(list)
        for u, v in edges:
            graph[u].append(v)
            graph[v].append(u)

        subtree_xor = [0] * n
        descendants = [set() for _ in range(n)]

        def dfs(node, parent):
            
            subtree_xor[node] = nums[node]
            descendants[node].add(node)
            
            for neighbor in graph[node]:
                if neighbor != parent:
                    dfs(neighbor, node)
                    
                    subtree_xor[node] ^= subtree_xor[neighbor]
                    descendants[node].update(descendants[neighbor])

        dfs(0, -1)
        
        total_xor = subtree_xor[0]
        min_score = float('inf')

        for i in range(1, n):
            for j in range(i + 1, n):
                xor_i = subtree_xor[i]
                xor_j = subtree_xor[j]
                
                if j in descendants[i]:
                    val1 = xor_j
                    val2 = xor_i ^ xor_j
                    val3 = total_xor ^ xor_i
                
                elif i in descendants[j]:
                    val1 = xor_i
                    val2 = xor_j ^ xor_i
                    val3 = total_xor ^ xor_j

                else:
                    val1 = xor_i
                    val2 = xor_j
                    val3 = total_xor ^ xor_i ^ xor_j
                
                score = max(val1, val2, val3) - min(val1, val2, val3)
                min_score = min(min_score, score)
                
        return min_score
